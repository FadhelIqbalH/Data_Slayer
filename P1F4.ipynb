{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-23T02:37:41.944988Z",
     "iopub.status.busy": "2024-12-23T02:37:41.944507Z",
     "iopub.status.idle": "2024-12-23T02:38:12.956978Z",
     "shell.execute_reply": "2024-12-23T02:38:12.955925Z",
     "shell.execute_reply.started": "2024-12-23T02:37:41.944944Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.23 (you have 1.4.21). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import Counter, defaultdict, deque\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import models, transforms\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T02:39:00.070851Z",
     "iopub.status.busy": "2024-12-23T02:39:00.070153Z",
     "iopub.status.idle": "2024-12-23T02:39:01.943129Z",
     "shell.execute_reply": "2024-12-23T02:39:01.942221Z",
     "shell.execute_reply.started": "2024-12-23T02:39:00.070809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_image_sequences(root_dir):\n",
    "    sequences = []\n",
    "    for subject in os.listdir(root_dir):\n",
    "        subject_path = os.path.join(root_dir, subject)\n",
    "        if not os.path.isdir(subject_path):\n",
    "            continue\n",
    "        for label in os.listdir(subject_path):\n",
    "            label_path = os.path.join(subject_path, label)\n",
    "            if not os.path.isdir(label_path):\n",
    "                continue\n",
    "            for action in os.listdir(label_path):\n",
    "                action_path = os.path.join(label_path, action)\n",
    "                if not os.path.isdir(action_path):\n",
    "                    continue\n",
    "                image_paths = sorted(glob(os.path.join(action_path, '*.jpg')))\n",
    "                if len(image_paths) == 0:\n",
    "                    continue\n",
    "                action_name = \"_\".join(action.split('_')[1:]) if '_' in action else action\n",
    "                sequences.append({\n",
    "                    'subject': subject,\n",
    "                    'label': label,\n",
    "                    'action': action_name,\n",
    "                    'image_paths': image_paths\n",
    "                })\n",
    "    return sequences\n",
    "\n",
    "root_dir = '/kaggle/input/data-slayer123/train/train'\n",
    "sequences = load_image_sequences(root_dir)\n",
    "\n",
    "fall_sequences = [s for s in sequences if s['label'] == 'fall']\n",
    "non_fall_sequences = [s for s in sequences if s['label'] == 'non_fall']\n",
    "\n",
    "train_fall, test_fall = train_test_split(fall_sequences, test_size=0.2, random_state=42)\n",
    "train_non_fall, test_non_fall = train_test_split(non_fall_sequences, test_size=0.2, random_state=42)\n",
    "\n",
    "train_sequences = train_fall + train_non_fall\n",
    "test_sequences = test_fall + test_non_fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T02:39:18.543056Z",
     "iopub.status.busy": "2024-12-23T02:39:18.542168Z",
     "iopub.status.idle": "2024-12-23T02:40:57.359742Z",
     "shell.execute_reply": "2024-12-23T02:40:57.358545Z",
     "shell.execute_reply.started": "2024-12-23T02:39:18.543016Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def apply_transformations(sequences, transform):\n",
    "    transformed_sequences = []\n",
    "    for seq in sequences:\n",
    "        transformed_images = []\n",
    "        for img_path in seq['image_paths']:\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            img = np.expand_dims(img, axis=2)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "            img = np.array(img, dtype=np.uint8)\n",
    "            augmented_img = transform(transforms.ToPILImage()(img))\n",
    "            transformed_images.append(augmented_img)\n",
    "\n",
    "        transformed_sequences.append({\n",
    "            'subject': seq['subject'],\n",
    "            'label': seq['label'],\n",
    "            'action': seq['action'],\n",
    "            'image_tensors': transformed_images\n",
    "        })\n",
    "    return transformed_sequences\n",
    "\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_augmented = apply_transformations(train_sequences, augmentation)\n",
    "test_transformed = apply_transformations(test_sequences, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T02:40:57.362674Z",
     "iopub.status.busy": "2024-12-23T02:40:57.361756Z",
     "iopub.status.idle": "2024-12-23T02:40:57.377330Z",
     "shell.execute_reply": "2024-12-23T02:40:57.376379Z",
     "shell.execute_reply.started": "2024-12-23T02:40:57.362634Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4, Image shape: torch.Size([3, 224, 224])\n",
      "Total single frame train samples: 3347\n",
      "Total single frame test samples: 884\n"
     ]
    }
   ],
   "source": [
    "class SingleFrameActionDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.data = []\n",
    "\n",
    "        for seq in sequences:\n",
    "            label = 1 if seq['label'] == 'fall' else 0\n",
    "            for img_tensor in seq['image_tensors']:\n",
    "                self.data.append((img_tensor, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        return image, label\n",
    "\n",
    "train_single_frame_dataset = SingleFrameActionDataset(train_augmented)\n",
    "test_single_frame_dataset = SingleFrameActionDataset(test_transformed)\n",
    "\n",
    "batch_size = 4\n",
    "train_single_frame_loader = DataLoader(train_single_frame_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_single_frame_loader = DataLoader(test_single_frame_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for images, labels in train_single_frame_loader:\n",
    "    assert images.shape[1:] == (3, 224, 224), f\"Shape mismatch in batch: {images.shape}\"\n",
    "    print(f\"Batch size: {images.shape[0]}, Image shape: {images.shape[1:]}\")\n",
    "    break\n",
    "\n",
    "print(f\"Total single frame train samples: {len(train_single_frame_dataset)}\")\n",
    "print(f\"Total single frame test samples: {len(test_single_frame_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T02:40:57.379046Z",
     "iopub.status.busy": "2024-12-23T02:40:57.378732Z",
     "iopub.status.idle": "2024-12-23T02:41:04.700033Z",
     "shell.execute_reply": "2024-12-23T02:41:04.698985Z",
     "shell.execute_reply.started": "2024-12-23T02:40:57.379015Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n",
      "100%|██████████| 21.1M/21.1M [00:00<00:00, 185MB/s]\n"
     ]
    }
   ],
   "source": [
    "class MobileNetV3Model(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super(MobileNetV3Model, self).__init__()\n",
    "\n",
    "        # MobileNetV3 Backbone\n",
    "        self.mobilenet = models.mobilenet_v3_large(pretrained=pretrained)\n",
    "        num_ftrs_mobilenet = self.mobilenet.classifier[0].in_features\n",
    "        self.mobilenet.classifier = nn.Identity()  # Remove the classifier\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs_mobilenet, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 2)  # Output layer for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mobilenet_features = self.mobilenet(x)\n",
    "        output = self.classifier(mobilenet_features)\n",
    "        return output\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MobileNetV3Model(pretrained=True).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T02:41:04.702096Z",
     "iopub.status.busy": "2024-12-23T02:41:04.701818Z",
     "iopub.status.idle": "2024-12-23T02:41:05.693994Z",
     "shell.execute_reply": "2024-12-23T02:41:05.692604Z",
     "shell.execute_reply.started": "2024-12-23T02:41:04.702069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4, 960]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m, in \u001b[0;36mComplexMobileNetV3Model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m mobilenet_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmobilenet(x)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Apply residual connection\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmobilenet_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Apply additional convolution layers\u001b[39;00m\n\u001b[1;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block(mobilenet_features)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4, 960]"
     ]
    }
   ],
   "source": [
    "# Loop pelatihan (contoh dengan 1 epoch)\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for images, labels in train_single_frame_loader:  # Pastikan loader untuk single frame digunakan\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_single_frame_loader)\n",
    "    epoch_accuracy = 100 * correct_train / total_train\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    # Update learning rate dengan scheduler\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-23T02:41:05.694721Z",
     "iopub.status.idle": "2024-12-23T02:41:05.695031Z",
     "shell.execute_reply": "2024-12-23T02:41:05.694889Z",
     "shell.execute_reply.started": "2024-12-23T02:41:05.694875Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class TestingDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Dataset untuk pengujian single frame dengan memuat semua gambar dari direktori.\n",
    "\n",
    "        Args:\n",
    "            image_dir (str): Path ke direktori yang berisi gambar uji.\n",
    "            transform (callable, optional): Transformasi opsional untuk diterapkan pada sampel.\n",
    "        \"\"\"\n",
    "        self.image_paths = glob(os.path.join(image_dir, '*'))  # Mendukung format gambar umum\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = -1  # Placeholder karena label tidak tersedia\n",
    "\n",
    "        # Membaca gambar dalam format grayscale\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Gambar tidak ditemukan atau tidak dapat dibaca: {img_path}\")\n",
    "\n",
    "        # Konversi grayscale ke RGB\n",
    "        img = np.expand_dims(img, axis=2)  # [H, W, 1]\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # [H, W, 3]\n",
    "        img = np.array(img, dtype=np.uint8)\n",
    "\n",
    "        # Konversi numpy array ke PIL.Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)  # [C, H, W]\n",
    "\n",
    "        return img, label, img_path\n",
    "\n",
    "# Path direktori test\n",
    "test_directory = '/kaggle/input/data-slayer123/test/test'  \n",
    "testing_dataset = TestingDataset(image_dir=test_directory, transform=test_transform)\n",
    "testing_loader = DataLoader(testing_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-23T02:41:05.696039Z",
     "iopub.status.idle": "2024-12-23T02:41:05.696315Z",
     "shell.execute_reply": "2024-12-23T02:41:05.696193Z",
     "shell.execute_reply.started": "2024-12-23T02:41:05.696179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_testing(model, testing_loader, device, num_samples=150, csv_path='predictions_single_frame.csv'):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the testing dataset, visualizes random samples, and saves predictions to CSV.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained PyTorch model.\n",
    "        testing_loader (DataLoader): DataLoader for the testing dataset.\n",
    "        device (torch.device): Device to perform computations on.\n",
    "        num_samples (int): Number of random samples to visualize.\n",
    "        csv_path (str): Path to save the CSV file with predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_paths = []\n",
    "\n",
    "    sample_images = []\n",
    "    sample_preds = []\n",
    "    sample_paths = []\n",
    "\n",
    "    total_samples = len(testing_loader.dataset)\n",
    "    num_samples = min(num_samples, total_samples)\n",
    "    random_indices = set(random.sample(range(total_samples), num_samples))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, _, img_paths) in enumerate(testing_loader):\n",
    "            # Move data to the specified device\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            pred_label = predicted.item()\n",
    "\n",
    "            img_path = img_paths[0]\n",
    "            img_filename = os.path.basename(img_path)\n",
    "\n",
    "            all_preds.append(pred_label)\n",
    "            all_paths.append(img_filename)\n",
    "\n",
    "            # Collect random samples for visualization\n",
    "            if batch_idx in random_indices and len(sample_images) < num_samples:\n",
    "                img = images[0].cpu().numpy()\n",
    "                img = np.transpose(img, (1, 2, 0))  # [H, W, C]\n",
    "                img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # Unnormalize\n",
    "                img = np.clip(img, 0, 1)\n",
    "                sample_images.append(img)\n",
    "                sample_preds.append(pred_label)\n",
    "                sample_paths.append(img_filename)\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    df = pd.DataFrame({\n",
    "        'id': all_paths,\n",
    "        'label': all_preds\n",
    "    })\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f'Predictions saved to {csv_path}')\n",
    "    \n",
    "    # Plot distribution of predictions\n",
    "    distribution = df['label'].value_counts().sort_index()\n",
    "    labels = ['NonFall', 'Fall']\n",
    "    counts = distribution.tolist()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(labels, counts, color=['blue', 'orange'])\n",
    "    plt.title('Distribution of Predictions')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    for i, count in enumerate(counts):\n",
    "        plt.text(i, count, str(count), ha='center', va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualize random samples\n",
    "    num_plots = len(sample_images)\n",
    "    cols = 15  # Update columns to 15 to fit more images per row (for 150 images, 15 x 10 grid)\n",
    "    rows = (num_plots + cols - 1) // cols  # Calculate rows needed to fit images\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 4 * rows))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(num_plots):\n",
    "        axes[i].imshow(sample_images[i])\n",
    "        axes[i].set_title(f'Path: {sample_paths[i]}\\nPredicted: {sample_preds[i]}')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    for i in range(num_plots, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate the model using the single frame test set\n",
    "evaluate_testing(model, testing_loader, device, num_samples=150, csv_path='predictions_single_frame9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-23T02:41:05.697266Z",
     "iopub.status.idle": "2024-12-23T02:41:05.697545Z",
     "shell.execute_reply": "2024-12-23T02:41:05.697424Z",
     "shell.execute_reply.started": "2024-12-23T02:41:05.697410Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions_path = \"/kaggle/working/predictions_single_frame9.csv\"\n",
    "sample_submission_path = \"/kaggle/input/sample/sample_submission.csv\"\n",
    "merged_output_path = \"P1F9.csv\"\n",
    "\n",
    "predictions = pd.read_csv(predictions_path)\n",
    "\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "merged = pd.merge(sample_submission, predictions, on='id', how='inner')\n",
    "\n",
    "merged = merged[['id', 'label_y']].rename(columns={'label_y': 'label'})\n",
    "merged.to_csv(merged_output_path, index=False)\n",
    "\n",
    "print(f'Merged CSV saved to {merged_output_path}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 6297557,
     "sourceId": 10192436,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6301467,
     "sourceId": 10198104,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6302387,
     "sourceId": 10199319,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
